{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas_mount/avinash_ocr/gs1chatbot/gs1_chatbot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 17 17:02:55 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 31%   25C    P8              11W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:40:00.0 Off |                  Off |\n",
      "| 42%   69C    P2             188W / 300W |  43782MiB / 49140MiB |     15%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:41:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8              20W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   25C    P8               2W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A     13392      C   ....conda/envs_dirs/atharva/bin/python     5888MiB |\n",
      "|    1   N/A  N/A     23725      C   python                                    37886MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\r\n",
      "Built on Fri_Nov__3_21:07:56_CDT_2017\r\n",
      "Cuda compilation tools, release 9.1, V9.1.85\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.43.1\n",
      "Uninstalling bitsandbytes-0.43.1:\n",
      "  Would remove:\n",
      "    /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages/bitsandbytes-0.43.1.dist-info/*\n",
      "    /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages/bitsandbytes/*\n",
      "    /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages/tests/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip3 install --upgrade bitsandbytes\n",
    "# !pip3 show bitsandbytes\n",
    "# !pip3 uninstall bitsandbytes\n",
    "\n",
    "pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.43.1\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: pyyaml in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade langchain\n",
    "# pip install transformers\n",
    "# pip install datasets\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas_mount/avinash_ocr/.conda/envs_dirs/llma1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# importing the packages\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "#import pinecone\n",
    "import warnings\n",
    "#from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "# from peft import LoraConfig, PeftModel\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# importing the packages\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer names\n",
    "#base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "#base_model_name=\"microsoft/phi-2\"\n",
    "\n",
    "#\"NousResearch/Llama-2-7b-chat-hf\"\n",
    "#refined_model = \"llama-2-7b-mlabonne-enhanced\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      " Unterscheidung von \"Hallo\" und \"How are you?\"\n",
      "\n",
      "In English, \"Hello\" is a common greeting used to acknowledge someone's presence, while \"How are you?\" is a question\n"
     ]
    }
   ],
   "source": [
    "# # Create the pipeline\n",
    "# inference_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=base_model,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# prompt = \"Hello, how are you?\"\n",
    "# generated_text = inference_pipeline(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# # Print the generated text\n",
    "# print(generated_text[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty = 1.5,\n",
    "    max_new_tokens = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1 : Simple Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What do you know about the sun?\n",
      "Assistant: Great question! The Sun is a star located at the center of our solar system, and it's truly amazing. Here are some interesting facts I could tell you... 🌞✨️\n",
      "1/8th Of Its Mass Is Composed Of Elemental Hydrogen And Helium; Together They Make Up About Three Quarters (75%) & Other Particles Like Nitrogen Are Present In Trace Amounts | #TheSunIsAmazING pic.twitter.com/QMuFz0CihO\n",
      "Elapsed Time: 6.909176826477051 seconds\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(\"Human: How old is the sun?\\nAssistant:\", return_tensors=\"pt\")\n",
    "\n",
    "query = \"What do you know about the sun?\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2 : Multi-Step Reasoning Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\n",
      "Assistant: Of course! Here's an affordable and delicious recipe for making chocolate vanilla bean cupcakes with cream cheese frosting using basic pantry staples that fits your requirements. Let me guide you through the steps below: - Budget Limitations ($4 or less per serving): To keep costs low while still producing high quality results I will use reduced fat dairy products such as 1/2 cups of plain Greek yogurt insteadof fullfat sourcremout $059 ($.63 /serving). AdditionallyI w ill us e Hershey '85 unsweetened appl...\n",
      "Elapsed Time: 8.049070119857788 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\"\n",
    "\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3 : Real-Time Interaction Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\n",
      "Assistant: Of course! I'd be happy to help with that. Let me just pull up some information on debugging techniques for beginners... *scrolls through notes* Okay, so here are some steps we can follow to debug this issue together. Firstly, let's start by reviewing your code carefully to identify any obvious errors or patterns. Can you tell us what line of code is causing trouble? Senior Developer: Great, thankyou very much assistant AIfor pullingup those notes, nice job 😊 . Now , could j u st tellme which partof thesoftware i s giving thee error ? Junior Dev : yea h _/ /th_ lways showstoolongnumber wheni try tot ranithe program . Senio rDev : Ahh, okaygot it !Wellthen,letssee whethermultiplying t he number b yhand givesus anythinguseful .JuniorDev:\\u0026 yeah, nothing happenswh eneither.Sen irDe v : Hm mminterest ing...Let ‘slink’ our way throught hisissue together .Step1w e c al llead-by -example .Firstt hen g etoutthe\n",
      "Elapsed Time: 14.604872226715088 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starling-LM-11B-alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS Token ID: 32000\n",
      "PAD Token ID: 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# importing the packages\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "#import pinecone\n",
    "import warnings\n",
    "#from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "# from peft import LoraConfig, PeftModel\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# importing the packages\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer names\n",
    "#base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model_name = \"CallComply/Starling-LM-11B-alpha\"\n",
    "#base_model_name=\"microsoft/phi-2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "###################################################################\n",
    "# Define EOS and PAD tokens explicitly if not already set\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Now both eos_token_id and pad_token_id should be set\n",
    "print(\"EOS Token ID:\", tokenizer.eos_token_id)\n",
    "print(\"PAD Token ID:\", tokenizer.pad_token_id)\n",
    "####################################################################\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty = 1.5,\n",
    "    max_new_tokens = 256,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1 : Simple Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What do you know about the sun?\n",
      "Assistant: The Sun is a G-type main sequence star located at near the center of the Solar System. It accounts for approximately 98%–99% (by mass) or roughly two thirds (by volume and luminosity in terms such as brightness, energy output or radius), with all other stars combined accounting for merely ~0.1 to 2% its total contribution; it forms part of an isolated binary system along with only one known companion that completes nearly exactly one orbit every 365 days, which itself takes almost precisely twice Jupiter's orbital period. Its proper motion relative to distant background objects results from its being carried around the galactic Center by the Milky Way’s gravitational pull within the local neighborhood where several dozen thousand neighboring nearby stars also move together much like beads strung on stringed necklaces wound up into compact spherical clusters..\n",
      "\n",
      "The Sun has no permanent polar magnetic field because convection currents continually reverse this direction causing them cancel out during each solar cycle making any net dipole moment zero due to large scale fluid motions creating turbulent mixing layer beneath photospheric level resulting from internal rotation rates varying greatly depending upon depth below surface layers exposed through granules seen observable phen\n",
      "Elapsed Time: 19.034393787384033 seconds\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(\"Human: How old is the sun?\\nAssistant:\", return_tensors=\"pt\")\n",
    "\n",
    "query = \"What do you know about the sun?\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2 : Multi-Step Reasoning Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\n",
      "Assistant: Step 1: Make sure you have all the necessary ingredients and equipment needed for your cakes before starting with baking process so that any unforeseen situation can be handled carefully by having backup items such as extra milk or sugar in case of accidental spillage etc.. Also keep ready some amount butter/oil for greasing purpose if required and also preheat oven while making preparations like mixing bowl , whisk & spatula / spoon handy near stove top . In fact try keeping everything at reachable distance from place where cooking will take happen ! It not only saves time but makes things easy too because every minute counts when one is busy doing multiple tasks simultaneously ;).   Here are steps according preparation which takes less costly yet delicious recipes :     \n",
      "Preparation Time –   20 min( includes chilling+mixing )Cooking Period -    35 mins Total Cost For One Cake Using All These Ingredients = Rs48 ( Considered On Average Price)      \n",
      "\n",
      "Ingredients Need To Be Used Are As Follows:-        Flour -60 gm OR Maida (All Purpose flour)=Rs7 Sugar=9; Milk Powder&Gur,(Jaggery); G\n",
      "Elapsed Time: 19.28829050064087 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\"\n",
    "\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3 : Real-Time Interaction Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\n",
      "Assistant: Here are some steps that can be followed as an outline for finding bugs or problems within your existing scripts/codes: 1) Reproduce it - Before anything else make sure this is happening consistently even outside of development mode if needed (ie; production vs dev version). Once reproduced reliably take note all variables at hand such as page load times, browser type & OS etc...2) Identify what specifically has gone wrong- Is there no data being returned? Wrong message coming up on screen after form submission instead of expected error handling?,3) Check every variable involved with scripting before execution begins then identify which part of script fails first / where exactly i.e., Javascript errors often come from syntax mistakes but also check whether everything seems properly initialized .4) Look into console logs any time something goes array ()  5 Use debugging tools provided via browsers themselves e..g Chrome Dev toolbar has breakpoints....6 ) For instance look through each line functionally testing one piece of logic at once , isolating areas causing problems may require commenting certain things off while running tests until final culprit found.\"This might seem obvious yet very easy way sometimes overlooked due complex nature issues especially when dealing large projects\".7 Try fixing small parts individually\n",
      "Elapsed Time: 20.190249919891357 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microsoft/phi-2.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.10s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# importing the packages\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "#import pinecone\n",
    "import warnings\n",
    "#from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "# from peft import LoraConfig, PeftModel\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# importing the packages\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer names\n",
    "#base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#base_model_name = \"CallComply/Starling-LM-11B-alpha\"\n",
    "base_model_name=\"microsoft/phi-2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty = 1.5,\n",
    "    max_new_tokens = 256,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1 : Simple Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What do you know about the sun?\n",
      "Assistant: I have learned that ________.\n",
      "Elapsed Time: 0.4977257251739502 seconds\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(\"Human: How old is the sun?\\nAssistant:\", return_tensors=\"pt\")\n",
    "\n",
    "query = \"What do you know about the sun?\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2 : Multi-Step Reasoning Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\n",
      "Assistant: 1) Determine the size and type of pan needed for your desired serving quantity (e.g., 9x13 inch). \n",
      "2) Research recipes that fit within your preferred flavors/custard base ingredients & sugar amounts using online resources such as cookbooks or cooking websites like Allrecipes etc. This may involve comparing multiple recipe results before making final decisions on which ones you will use because each person has different preferences when it comes down with flavor variation's in their desserts(i think vanilla is popular but other people might prefer chocolate!). 3)? Add all necessary kitchen supplies into shopping list format so nothing gets overlooked during preparation time! 4?). Make sure everything fits comfortably under budget constraints without sacrificing quality by eliminating any unnecessary items; don't buy anything if there isn’t enough money available at this moment – instead save up until later dates once finances have improved.. 5 ). Purchase materials ahead based upon what can be found most easily nearby since availability varies depending where one lives - grocery stores are often better options than specialty shops due lower prices per item set plus wider selection ; check local newspapers weekly edition listing store opening hours too, also important factor while planning purchasing activity prior purchase\n",
      "Elapsed Time: 10.663727521896362 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Prepare a plan how to make a cake, considering budget limitations, ingredient preference. Generate a step-by-step answer.\"\n",
    "\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3 : Real-Time Interaction Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\n",
      "Assistant: 1) Open up your editor or IDE where the error occurred  2️⃣ Find all relevant lines of codes within 10-15 minutes 3🖥 Test each line thoroughly for errors 4👆 Use online forums/resources when stuck 5😊 Review similar issues solved effectively on Stack Overflow 6❌ If nothing was found after 20 mins 🙏 Try running debugging tools 7&8™ Consult colleagues if needed 9–10 - Fixing bugs is an iterative process that requires patience but it's key part being able work through them efficiently! 11 – Share successful fixes with other developers 12 & 13— Keep track log files 14 — Reflect back 15° Finally enjoy what works well 😁\n",
      "Elapsed Time: 6.6301305294036865 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"Imagine you're a senior developer helping a junior developer fix a bug in a web app. Explain how to find and fix the problem step by step, like checking the code and trying out different solutions.\"\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "inputs = tokenizer(\"Human: \"+query+\"\\nAssistant:\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "# passing the prompt tokens to the generative model to generate the response tokens\n",
    "with torch.no_grad():\n",
    "    generation_output = base_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=torch.ones_like(input_ids),\n",
    "        generation_config=generation_config,\n",
    "    )    \n",
    "    \n",
    "# decode the model generated tokens, and printing the generated output.\n",
    "output_text = tokenizer.decode(generation_output[0].cuda(), skip_special_tokens=True).strip()\n",
    "print(output_text)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
